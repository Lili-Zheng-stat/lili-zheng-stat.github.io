<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research by Topic</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="service.html">Professional&nbsp;Experiences</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research by Topic</h1>
</div>
<h2>Graphical Models: Methods and Theory with Missing Data</h2>
<p>High-dimensional graphical models have been a powerful tool for learning connections or interaction patterns
among a large number of variables, with wide applications such as learning stock networks, social networks, etc. While most prior
work focuses on the case when all variables are measured
simultaneously, one typical challenge in real data sets is that 
<b>only certain subsets of variables can be measured together, or can be measured sufficiently many times</b>.
To estimate the graph (conditional independence relationship) or certain characteristics of the graph accurately, novel statistical methods and theory 
need to be developed. I am actively working on this direction and happy to collaborate on related topics!</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/pdf/2210.11625">Graphical Model Inference with Erosely Measured Data</a> <br />
<b>Lili Zheng</b>, Genevera I. Allen <br />
<i>Accepted to Journal of the American Statistical Association, Theory and Methods</i> <br />
In this work, we are primarily concerned with graphical model inference from uneven and irregular measurements, which we term as &lsquo;&lsquo;erose measurements". This is motivated by neuroscience and genetic data applications where the missingness can be highly uneven with drastically different sample sizes. In these scenarios, uncertainty quantification can be extremely important since some parts of the graph can be estimated with much higher confidence than the others. We propose <b>GI-JOE</b> (Graph <b>Inference when Joint Observations are Erose) to perform edge-wise testing in this setting, where the uncertainty level of each edge depends on the sample size of the associated neighbors</b>. <i>Below is a illustrative example of how GI-JOE (tested graph on the right) improves graph selection by considering uneven uncertainties across the graph.</i> <br />
<img src="fig/GIJOE.jpeg" width="900px" height="300px" alt="alt text" /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.13491">Nonparanormal Graph Quilting with Applications to Calcium Imaging</a> <br />
Andersen Chang*, <b>Lili Zheng</b>*, Gautam Dasarthy, Genevera I. Allen <br />
<i>Accepted to STAT</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2209.08273">Low-Rank Covariance Completion for Graph Quilting with Applications to Functional Connectivity</a> <br />
Andersen Chang, <b>Lili Zheng</b>, Genevera I. Allen <br />
<i>Under revision at Journal of the American Statistical Association, Applications and Case Studies</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9746583">Learning Gaussian Graphical Models with Differing Pairwise Sample Sizes</a> <br />
<b>Lili Zheng</b>, Genevera I. Allen <br />
<i>International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2022</i></p>
</li>
</ul>
<h2>Interpretable Machine Learning</h2>
<p>With machine learning models being implemented everywhere in modern life, making them interpretable and trustworthy is a crucial task for researchers from different domains. As a statistician, I am passionate about contributing to the challenging problems in interpretable machine learning through statistical lens, e.g., statistical theory and inference methods.</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.13491">Interpretable Machine Learning for Discovery: Statistical Challenges &amp; Opportunities</a> <br />
Genevera I. Allen, Luqin Gan, <b>Lili Zheng</b> <br />
<i>Under revision at Annual Review of Statistics and Its Application</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2206.02088">Model-Agnostic Confidence Intervals for Feature Importance: A Fast and Powerful Approach Using Minipatch Ensembles</a> <br />
Luqin Gan*, <b>Lili Zheng</b>*, Genevera I. Allen <br />
<i>Submitted</i></p>
</li>
</ul>
<h2>High-dimensional Networks Estimation in Time Series Models</h2>
<p>High-dimensional autoregressive models can capture how the past events<i>status associated with a huge collection of nodes influence their future events</i>status, where the influence patterns can reveal underlying network structures. For example, the past firing of neurons may trigger or inhibit the future firings of their neighbors; past posts of a twitter user may also influence the likelihood of his/her followers to send new tweets. The influence network among these nodes can then be encoded by the high-dimensional autoregressive parameter. <b>The estimation and testing problem for the underlying network structure</b> imposes both methodological and theoretical challenges.</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2003.07429">Context-dependent self-exciting point processes: models, methods, and risk bounds in high dimensions</a> <br />
<b>Lili Zheng</b>, Garvesh Raskutti, Rebecca Willett, Benjamin Mark <br />
<i>Journal of Machine Learning Research. 2021</i> [<a href="pre/context-dependent-slides">Slides</a>][<a href="https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models">Code</a>] <br />
In this work, we propose two autoregressive models with corresponding methods and theory for learning 
context-dependent networks that reflect how features associated with an event (such as the content of a social media post) modulate the strength of influences among nodes. The multinomial approach we propose is suited to categorical marks and while the logistic-normal approach is suited to marks with mixed membership in different categories; a mixture approach is also proposed to combine the merits of both methods. The following figure provides a comparison among the three approaches. <br />
<img src="fig/context-dependent-synthetic.png" width="450px" height="560px" alt="alt text" /></p>
</li>
<li><p><a href="https://projecteuclid.org/euclid.ejs/1576119708">Testing for high-dimensional network parameters in auto-regressive models</a> <br />
<b>Lili Zheng</b>, Garvesh Raskutti <br />
<i>Electronic Journal of Statistics. 2019</i> [<a href="https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models">Code</a>] <br />
Below is an example of the hypothesis testing results of our method on Chicago crime data, where the goal is to test which community's past crimes has significant influence upon another community's future crimes. All communities involved in significant edges are colored, showing geographical approximity.<br /></p>
</li>
</ul>
<p><img src="fig/hptest-chicago.png" width="300px" height="400px" alt="alt text" /></p>
<h2>Tensor Data Analysis</h2>
<p>Tensor data has attracted wide interest in recent years since it contains valuable high-order information,
while its high-dimensionality imposes numerous statistical and computational challenges. One of my research interest is to develop <b>efficient and statistically accurate algorithms for solving real-world tensor problems</b>.</p>
<p>Papers:</p>
<ul>
<li><p><a href="JisstPCA.pdf">Joint Semi-Symmetric Tensor PCA for Integrating Multi-modal Populations of Networks</a><br />
Jiaming Liu*, <b>Lili Zheng</b>*, Zhengwu Zhang, Genevera I. Allen <br />
<i>Submitted</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9820079/">A Low-Rank Tensor Completion Approach for Imputing Functional Neuronal Data from Multiple Recordings</a> <br />
<b>Lili Zheng</b>; Zachary T. Rewolinski; Genevera I. Allen <br />
<i>IEEE Data Science and Learning Workshop (DSLW). 2022</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2010.02482">Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration</a> <br />
Yuchen Zhou, Anru R. Zhang, <b>Lili Zheng</b>, Yazhen Wang <br />
<i>IEEE Transactions on Information Theory. 2022</i> [<a href="https://github.com/Lili-Zheng-stat/TTOI">Code</a>]<br /></p>
</li>
</ul>
<h2>Non-convex Optimization</h2>
<p>Non-convex optimization problems arise frequently from both modern machine learning algorithms (e.g., deep neural networks and Gaussian processes) and complex data structures (missing data). Although being challenging solely from an optimization perspective, these problems can often lend a helping hand from certain statistical modeling. I am interested in the intersection between statistics and optimization, especially when efficient non-convex algorithms can still exhibit strong statistical performance.</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2304.09305">High-dimensional Multi-class Classification with Presence-only Data</a> <br />
<b>Lili Zheng</b>, Garvesh Raskutti <br />
<i>Under revision at Electronic Journal of Statistics</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://www.jmlr.org/papers/volume23/20-1365/20-1365.pdf">Gaussian Process Parameter
Estimation Using Mini-batch Stochastic Gradient Descent: Convergence Guarantees and Empirical
Benefits</a> <br />
Hao Chen*, <b>Lili Zheng</b>*, Raed Al Kontar, Garvesh Raskutti <br />
<i>Journal of Machine Learning Research, 2022</i> <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/1cb524b5a3f3f82be4a7d954063c07e2-Paper.pdf">Stochastic Gradient Descent in Correlated Settings: A Study on Gaussian Processes</a> <br />
Hao Chen*, <b>Lili Zheng</b>*, Raed Al Kontar, Garvesh Raskutti <br />
<i>Neural Information Processing Systems (NeurIPS). 2020</i> [<a href="pre/SGD-GP_recording.mov">Video</a>][<a href="https://github.com/UMDataScienceLab/SGD-in-Gaussain-processes">Code</a>] <br /></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-08-27 23:24:58 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
